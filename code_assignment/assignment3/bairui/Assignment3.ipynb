{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def data_split(data, test_rate=0.3):\n",
    "    \"\"\" Take some data , and split them into training set and test set.\"\"\"\n",
    "    train = list()\n",
    "    test = list()\n",
    "    i = 0\n",
    "    for datum in data:\n",
    "        i += 1\n",
    "        if random.random() > test_rate:\n",
    "            train.append(datum)\n",
    "        else:\n",
    "            test.append(datum)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "class Random_embedding():\n",
    "    def __init__(self, data, test_rate=0.3):\n",
    "        self.dict_words = dict()\n",
    "        _data = [item.split('\\t') for item in data]\n",
    "        self.data = [[item[5], item[6], item[0]] for item in _data]\n",
    "        self.data.sort(key=lambda x:len(x[0].split()))\n",
    "        self.len_words = 0\n",
    "        self.train, self.test = data_split(self.data, test_rate=test_rate)\n",
    "        self.type_dict = {'-': 0, 'contradiction': 1, 'entailment': 2, 'neutral': 3}\n",
    "        self.train_y = [self.type_dict[term[2]] for term in self.train]  # Relation in training set\n",
    "        self.test_y = [self.type_dict[term[2]] for term in self.test]  # Relation in test set\n",
    "        self.train_s1_matrix = list()\n",
    "        self.test_s1_matrix = list()\n",
    "        self.train_s2_matrix = list()\n",
    "        self.test_s2_matrix = list()\n",
    "        self.longest = 0\n",
    "\n",
    "    def get_words(self):\n",
    "        pattern = '[A-Za-z|\\']+'\n",
    "        for term in self.data:\n",
    "            for i in range(2):\n",
    "                s = term[i]\n",
    "                s = s.upper()\n",
    "                words = re.findall(pattern, s)\n",
    "                for word in words:  # Process every word\n",
    "                    if word not in self.dict_words:\n",
    "                        self.dict_words[word] = len(self.dict_words)+1\n",
    "        self.len_words = len(self.dict_words)\n",
    "\n",
    "    def get_id(self):\n",
    "        pattern = '[A-Za-z|\\']+'\n",
    "        for term in self.train:\n",
    "            s = term[0]\n",
    "            s = s.upper()\n",
    "            words = re.findall(pattern, s)\n",
    "            item = [self.dict_words[word] for word in words]\n",
    "            self.longest = max(self.longest, len(item))\n",
    "            self.train_s1_matrix.append(item)\n",
    "            s = term[1]\n",
    "            s = s.upper()\n",
    "            words = re.findall(pattern, s)\n",
    "            item = [self.dict_words[word] for word in words]\n",
    "            self.longest = max(self.longest, len(item))\n",
    "            self.train_s2_matrix.append(item)\n",
    "        for term in self.test:\n",
    "            s = term[0]\n",
    "            s = s.upper()\n",
    "            words = re.findall(pattern, s)\n",
    "            item = [self.dict_words[word] for word in words]\n",
    "            self.longest = max(self.longest, len(item))\n",
    "            self.test_s1_matrix.append(item)\n",
    "            s = term[1]\n",
    "            s = s.upper()\n",
    "            words = re.findall(pattern, s)\n",
    "            item = [self.dict_words[word] for word in words]\n",
    "            self.longest = max(self.longest, len(item))\n",
    "            self.test_s2_matrix.append(item)\n",
    "        self.len_words+=1\n",
    "\n",
    "\n",
    "class Glove_embedding():\n",
    "    def __init__(self, data, trained_dict, test_rate=0.3):\n",
    "        self.dict_words = dict()\n",
    "        _data = [item.split('\\t') for item in data]\n",
    "        self.data = [[item[5], item[6], item[0]] for item in _data]\n",
    "        self.data.sort(key=lambda x:len(x[0].split()))\n",
    "        self.trained_dict = trained_dict\n",
    "        self.len_words = 0\n",
    "        self.train, self.test = data_split(self.data, test_rate=test_rate)\n",
    "        self.type_dict = {'-': 0, 'contradiction': 1, 'entailment': 2, 'neutral': 3}\n",
    "        self.train_y = [self.type_dict[term[2]] for term in self.train]  # Relation in training set\n",
    "        self.test_y = [self.type_dict[term[2]] for term in self.test]  # Relation in test set\n",
    "        self.train_s1_matrix = list()\n",
    "        self.test_s1_matrix = list()\n",
    "        self.train_s2_matrix = list()\n",
    "        self.test_s2_matrix = list()\n",
    "        self.longest = 0\n",
    "        self.embedding = list()\n",
    "\n",
    "    def get_words(self):\n",
    "        self.embedding.append([0]*50)\n",
    "        pattern = '[A-Za-z|\\']+'\n",
    "        for term in self.data:\n",
    "            for i in range(2):\n",
    "                s = term[i]\n",
    "                s = s.upper()\n",
    "                words = re.findall(pattern, s)\n",
    "                for word in words:  # Process every word\n",
    "                    if word not in self.dict_words:\n",
    "                        self.dict_words[word] = len(self.dict_words)\n",
    "                        if word in self.trained_dict:\n",
    "                            self.embedding.append(self.trained_dict[word])\n",
    "                        else:\n",
    "                            # print(word)\n",
    "                            # raise Exception(\"words not found!\")\n",
    "                            self.embedding.append([0] * 50)\n",
    "        self.len_words = len(self.dict_words)\n",
    "\n",
    "    def get_id(self):\n",
    "        pattern = '[A-Za-z|\\']+'\n",
    "        for term in self.train:\n",
    "            s = term[0]\n",
    "            s = s.upper()\n",
    "            words = re.findall(pattern, s)\n",
    "            item = [self.dict_words[word] for word in words]\n",
    "            self.longest = max(self.longest, len(item))\n",
    "            self.train_s1_matrix.append(item)\n",
    "            s = term[1]\n",
    "            s = s.upper()\n",
    "            words = re.findall(pattern, s)\n",
    "            item = [self.dict_words[word] for word in words]\n",
    "            self.longest = max(self.longest, len(item))\n",
    "            self.train_s2_matrix.append(item)\n",
    "        for term in self.test:\n",
    "            s = term[0]\n",
    "            s = s.upper()\n",
    "            words = re.findall(pattern, s)\n",
    "            item = [self.dict_words[word] for word in words]\n",
    "            self.longest = max(self.longest, len(item))\n",
    "            self.test_s1_matrix.append(item)\n",
    "            s = term[1]\n",
    "            s = s.upper()\n",
    "            words = re.findall(pattern, s)\n",
    "            item = [self.dict_words[word] for word in words]\n",
    "            self.longest = max(self.longest, len(item))\n",
    "            self.test_s2_matrix.append(item)\n",
    "        self.len_words+=1\n",
    "\n",
    "\n",
    "class ClsDataset(Dataset):\n",
    "    \"\"\" 文本分类数据集 \"\"\"\n",
    "    def __init__(self, sentence1,sentence2, relation):\n",
    "        self.sentence1 = sentence1\n",
    "        self.sentence2 = sentence2\n",
    "        self.relation = relation\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.sentence1[item], self.sentence2[item],self.relation[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.relation)\n",
    "\n",
    "\n",
    "def collate_fn(batch_data):\n",
    "    \"\"\" 自定义一个batch里面的数据的组织方式 \"\"\"\n",
    "\n",
    "    sents1,sents2, labels = zip(*batch_data)\n",
    "    sentences1 = [torch.LongTensor(sent) for sent in sents1]\n",
    "    padded_sents1 = pad_sequence(sentences1, batch_first=True, padding_value=0)\n",
    "    sentences2 = [torch.LongTensor(sent) for sent in sents2]\n",
    "    padded_sents2 = pad_sequence(sentences2, batch_first=True, padding_value=0)\n",
    "    return torch.LongTensor(padded_sents1), torch.LongTensor(padded_sents2),  torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "def get_batch(x1,x2,y,batch_size):\n",
    "    dataset = ClsDataset(x1,x2, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,drop_last=True,collate_fn=collate_fn)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Input_Encoding(nn.Module):\n",
    "    def __init__(self, len_feature, len_hidden, len_words,longest, weight=None, layer=1, batch_first=True, drop_out=0.5):\n",
    "        super(Input_Encoding, self).__init__()\n",
    "        self.len_feature = len_feature\n",
    "        self.len_hidden = len_hidden\n",
    "        self.len_words = len_words\n",
    "        self.layer = layer\n",
    "        self.longest=longest\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        if weight is None:\n",
    "            x = nn.init.xavier_normal_(torch.Tensor(len_words, len_feature))\n",
    "            self.embedding = nn.Embedding(num_embeddings=len_words, embedding_dim=len_feature, _weight=x).to(device)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(num_embeddings=len_words, embedding_dim=len_feature, _weight=weight).to(device)\n",
    "        self.lstm = nn.LSTM(input_size=len_feature, hidden_size=len_hidden, num_layers=layer, batch_first=batch_first,\n",
    "                            bidirectional=True).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.LongTensor(x).to(device)\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        self.lstm.flatten_parameters()\n",
    "        x, _ = self.lstm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Local_Inference_Modeling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Local_Inference_Modeling, self).__init__()\n",
    "        self.softmax_1 = nn.Softmax(dim=1).to(device)\n",
    "        self.softmax_2 = nn.Softmax(dim=2).to(device)\n",
    "\n",
    "    def forward(self, a_bar, b_bar):\n",
    "        e = torch.matmul(a_bar, b_bar.transpose(1, 2)).to(device)\n",
    "\n",
    "        a_tilde = self.softmax_2(e)\n",
    "        a_tilde = a_tilde.bmm(b_bar)\n",
    "        b_tilde = self.softmax_1(e)\n",
    "        b_tilde = b_tilde.transpose(1, 2).bmm(a_bar)\n",
    "\n",
    "        m_a = torch.cat([a_bar, a_tilde, a_bar - a_tilde, a_bar * a_tilde], dim=-1)\n",
    "        m_b = torch.cat([b_bar, b_tilde, b_bar - b_tilde, b_bar * b_tilde], dim=-1)\n",
    "\n",
    "        return m_a, m_b\n",
    "\n",
    "\n",
    "class Inference_Composition(nn.Module):\n",
    "    def __init__(self, len_feature, len_hidden_m, len_hidden, layer=1, batch_first=True, drop_out=0.5):\n",
    "        super(Inference_Composition, self).__init__()\n",
    "        self.linear = nn.Linear(len_hidden_m, len_feature).to(device)\n",
    "        self.lstm = nn.LSTM(input_size=len_feature, hidden_size=len_hidden, num_layers=layer, batch_first=batch_first,\n",
    "                            bidirectional=True).to(device)\n",
    "        self.dropout = nn.Dropout(drop_out).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        self.lstm.flatten_parameters()\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Prediction(nn.Module):\n",
    "    def __init__(self, len_v, len_mid, type_num=4, drop_out=0.5):\n",
    "        super(Prediction, self).__init__()\n",
    "        self.mlp = nn.Sequential(nn.Dropout(drop_out), nn.Linear(len_v, len_mid), nn.Tanh(),\n",
    "                                 nn.Linear(len_mid, type_num)).to(device)\n",
    "\n",
    "    def forward(self, a,b):\n",
    "\n",
    "        v_a_avg=a.sum(1)/a.shape[1]\n",
    "        v_a_max = a.max(1)[0]\n",
    "\n",
    "        v_b_avg = b.sum(1) / b.shape[1]\n",
    "        v_b_max = b.max(1)[0]\n",
    "\n",
    "        out_put = torch.cat((v_a_avg, v_a_max,v_b_avg,v_b_max), dim=-1)\n",
    "\n",
    "        return self.mlp(out_put)\n",
    "\n",
    "\n",
    "class ESIM(nn.Module):\n",
    "    def __init__(self, len_feature, len_hidden, len_words,longest, type_num=4, weight=None, layer=1, batch_first=True,\n",
    "                 drop_out=0.5):\n",
    "        super(ESIM, self).__init__()\n",
    "        self.len_words=len_words\n",
    "        self.longest=longest\n",
    "        self.input_encoding = Input_Encoding(len_feature, len_hidden, len_words,longest, weight=weight, layer=layer,\n",
    "                                             batch_first=batch_first, drop_out=drop_out)\n",
    "        self.local_inference_modeling = Local_Inference_Modeling()\n",
    "        self.inference_composition = Inference_Composition(len_feature, 8 * len_hidden, len_hidden, layer=layer,\n",
    "                                                           batch_first=batch_first, drop_out=drop_out)\n",
    "        self.prediction=Prediction(len_hidden*8,len_hidden,type_num=type_num,drop_out=drop_out)\n",
    "\n",
    "    def forward(self,a,b):\n",
    "        a_bar=self.input_encoding(a)\n",
    "        b_bar=self.input_encoding(b)\n",
    "\n",
    "        m_a,m_b=self.local_inference_modeling(a_bar,b_bar)\n",
    "\n",
    "        v_a=self.inference_composition(m_a)\n",
    "        v_b=self.inference_composition(m_b)\n",
    "\n",
    "        out_put=self.prediction(v_a,v_b)\n",
    "\n",
    "        return out_put\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "\n",
    "def NN_embdding(model, train, test, learning_rate, iter_times):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fun = F.cross_entropy\n",
    "    train_loss_record = list()\n",
    "    test_loss_record = list()\n",
    "    train_record = list()\n",
    "    test_record = list()\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    for iteration in range(iter_times):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train):\n",
    "            torch.cuda.empty_cache()\n",
    "            x1, x2, y = batch\n",
    "            pred = model(x1, x2).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y=y.to(device)\n",
    "            loss = loss_fun(pred, y).to(device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_acc = list()\n",
    "        test_acc = list()\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        for i, batch in enumerate(train):\n",
    "            torch.cuda.empty_cache()\n",
    "            x1, x2, y = batch\n",
    "            y=y.to(device)\n",
    "            pred = model(x1, x2).to(device)\n",
    "            loss = loss_fun(pred, y).to(device)\n",
    "            train_loss += loss.item()\n",
    "            _, y_pre = torch.max(pred, -1)\n",
    "            acc = torch.mean((torch.tensor(y_pre == y, dtype=torch.float)))\n",
    "            train_acc.append(acc)\n",
    "\n",
    "        for i, batch in enumerate(test):\n",
    "            torch.cuda.empty_cache()\n",
    "            x1, x2, y = batch\n",
    "            y=y.to(device)\n",
    "            pred = model(x1, x2).to(device)\n",
    "            loss = loss_fun(pred, y).to(device)\n",
    "            test_loss += loss.item()\n",
    "            _, y_pre = torch.max(pred, -1)\n",
    "            acc = torch.mean((torch.tensor(y_pre == y, dtype=torch.float)))\n",
    "            test_acc.append(acc)\n",
    "\n",
    "        trains_acc = sum(train_acc) / len(train_acc)\n",
    "        tests_acc = sum(test_acc) / len(test_acc)\n",
    "\n",
    "        train_loss_record.append(train_loss / len(train_acc))\n",
    "        test_loss_record.append(test_loss/ len(test_acc))\n",
    "        train_record.append(trains_acc.cpu())\n",
    "        test_record.append(tests_acc.cpu())\n",
    "        print(\"---------- Iteration\", iteration + 1, \"----------\")\n",
    "        print(\"Train loss:\", train_loss/ len(train_acc))\n",
    "        print(\"Test loss:\", test_loss/ len(test_acc))\n",
    "        print(\"Train accuracy:\", trains_acc)\n",
    "        print(\"Test accuracy:\", tests_acc)\n",
    "\n",
    "    return train_loss_record, test_loss_record, train_record, test_record\n",
    "\n",
    "\n",
    "def NN_plot(random_embedding, glove_embedding, len_feature, len_hidden, learning_rate, batch_size, iter_times):\n",
    "    train_random = get_batch(random_embedding.train_s1_matrix, random_embedding.train_s2_matrix,\n",
    "                             random_embedding.train_y,batch_size)\n",
    "    test_random = get_batch(random_embedding.test_s1_matrix, random_embedding.test_s2_matrix,\n",
    "                            random_embedding.test_y,batch_size)\n",
    "    train_glove = get_batch(glove_embedding.train_s1_matrix, glove_embedding.train_s2_matrix,\n",
    "                            glove_embedding.train_y,batch_size)\n",
    "    test_glove = get_batch(glove_embedding.test_s1_matrix, glove_embedding.test_s2_matrix,\n",
    "                           glove_embedding.test_y,batch_size)\n",
    "    random.seed(2021)\n",
    "    numpy.random.seed(2021)\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    random_model = ESIM(len_feature, len_hidden, random_embedding.len_words, longest=random_embedding.longest)\n",
    "    random.seed(2021)\n",
    "    numpy.random.seed(2021)\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    glove_model = ESIM(len_feature, len_hidden, glove_embedding.len_words, longest=glove_embedding.longest,\n",
    "                       weight=torch.tensor(glove_embedding.embedding, dtype=torch.float))\n",
    "    random.seed(2021)\n",
    "    numpy.random.seed(2021)\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    trl_ran, tsl_ran, tra_ran, tea_ran = NN_embdding(random_model, train_random, test_random, learning_rate,\n",
    "                                                     iter_times)\n",
    "    random.seed(2021)\n",
    "    numpy.random.seed(2021)\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    trl_glo, tsl_glo, tra_glo, tea_glo = NN_embdding(glove_model, train_glove, test_glove, learning_rate,\n",
    "                                                     iter_times)\n",
    "    x = list(range(1, iter_times + 1))\n",
    "    matplotlib.pyplot.subplot(2, 2, 1)\n",
    "    matplotlib.pyplot.plot(x, trl_ran, 'r--', label='random')\n",
    "    matplotlib.pyplot.plot(x, trl_glo, 'g--', label='glove')\n",
    "    matplotlib.pyplot.legend(fontsize=10)\n",
    "    matplotlib.pyplot.title(\"Train Loss\")\n",
    "    matplotlib.pyplot.xlabel(\"Iterations\")\n",
    "    matplotlib.pyplot.ylabel(\"Loss\")\n",
    "    matplotlib.pyplot.subplot(2, 2, 2)\n",
    "    matplotlib.pyplot.plot(x, tsl_ran, 'r--', label='random')\n",
    "    matplotlib.pyplot.plot(x, tsl_glo, 'g--', label='glove')\n",
    "    matplotlib.pyplot.legend(fontsize=10)\n",
    "    matplotlib.pyplot.title(\"Test Loss\")\n",
    "    matplotlib.pyplot.xlabel(\"Iterations\")\n",
    "    matplotlib.pyplot.ylabel(\"Loss\")\n",
    "    matplotlib.pyplot.subplot(2, 2, 3)\n",
    "    matplotlib.pyplot.plot(x, tra_ran, 'r--', label='random')\n",
    "    matplotlib.pyplot.plot(x, tra_glo, 'g--', label='glove')\n",
    "    matplotlib.pyplot.legend(fontsize=10)\n",
    "    matplotlib.pyplot.title(\"Train Accuracy\")\n",
    "    matplotlib.pyplot.xlabel(\"Iterations\")\n",
    "    matplotlib.pyplot.ylabel(\"Accuracy\")\n",
    "    matplotlib.pyplot.ylim(0, 1)\n",
    "    matplotlib.pyplot.subplot(2, 2, 4)\n",
    "    matplotlib.pyplot.plot(x, tea_ran, 'r--', label='random')\n",
    "    matplotlib.pyplot.plot(x, tea_glo, 'g--', label='glove')\n",
    "    matplotlib.pyplot.legend(fontsize=10)\n",
    "    matplotlib.pyplot.title(\"Test Accuracy\")\n",
    "    matplotlib.pyplot.xlabel(\"Iterations\")\n",
    "    matplotlib.pyplot.ylabel(\"Accuracy\")\n",
    "    matplotlib.pyplot.ylim(0, 1)\n",
    "    matplotlib.pyplot.tight_layout()\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(8, 8, forward=True)\n",
    "    matplotlib.pyplot.savefig('main_plot.jpg')\n",
    "    matplotlib.pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open('snli_1.0_train.txt', 'r') as f:\n",
    "    temp = f.readlines()\n",
    "\n",
    "with open('glove.6B.50d.txt', 'rb') as f:  # for glove embedding\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Construct dictionary with glove\n",
    "\n",
    "trained_dict = dict()\n",
    "n = len(lines)\n",
    "for i in range(n):\n",
    "    line = lines[i].split()\n",
    "    trained_dict[line[0].decode(\"utf-8\").upper()] = [float(line[j]) for j in range(1, 51)]\n",
    "\n",
    "data = temp[1:]\n",
    "# max_item = 100000\n",
    "# data = data[:max_item]\n",
    "learning_rate = 0.001\n",
    "len_feature = 50\n",
    "len_hidden = 50\n",
    "iter_times = 50\n",
    "batch_size = 1000\n",
    "\n",
    "# random embedding\n",
    "random.seed(2021)\n",
    "random_embedding = Random_embedding(data=data)\n",
    "random_embedding.get_words()\n",
    "random_embedding.get_id()\n",
    "\n",
    "# trained embedding : glove\n",
    "random.seed(2021)\n",
    "glove_embedding = Glove_embedding(data=data, trained_dict=trained_dict)\n",
    "glove_embedding.get_words()\n",
    "glove_embedding.get_id()\n",
    "\n",
    "NN_plot(random_embedding, glove_embedding, len_feature, len_hidden, learning_rate, batch_size, iter_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
